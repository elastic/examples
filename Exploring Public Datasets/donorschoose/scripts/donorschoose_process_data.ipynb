{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "### Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import elasticsearch\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from elasticsearch import helpers\n",
    "from time import perf_counter\n",
    "import concurrent\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "#from numba import jit\n",
    "\n",
    "# Define elasticsearch class\n",
    "es = elasticsearch.Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions\n",
    "# convert np.int64 into int. json.dumps does not work with int64\n",
    "class SetEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.int64):\n",
    "            return np.int(obj)\n",
    "        # else\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# Convert datestamp into ISO format\n",
    "def str_to_iso(text):\n",
    "    if text != '':\n",
    "        for fmt in ('%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d'):\n",
    "            try:\n",
    "                return datetime.isoformat(datetime.strptime(text, fmt))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        raise ValueError('no valid date format found : '.format(text))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Custom groupby function\n",
    "def concatdf(x):\n",
    "    if len(x) > 1:  #if multiple values\n",
    "        return list(x)\n",
    "    else: #if single value\n",
    "        return x.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Data\n",
    "# Load projects, resources & donations data\n",
    "print(\"Loading datasets\")\n",
    "start = perf_counter()\n",
    "projects = pd.read_csv('./data/opendata_projects000.gz', escapechar='\\\\', names=['projectid', 'teacher_acctid', 'schoolid', 'school_ncesid', 'school_latitude', 'school_longitude', 'school_city', 'school_state', 'school_zip', 'school_metro', 'school_district', 'school_county', 'school_charter', 'school_magnet', 'school_year_round', 'school_nlns', 'school_kipp', 'school_charter_ready_promise', 'teacher_prefix', 'teacher_teach_for_america', 'teacher_ny_teaching_fellow', 'primary_focus_subject', 'primary_focus_area' ,'secondary_focus_subject', 'secondary_focus_area', 'resource_type', 'poverty_level', 'grade_level', 'vendor_shipping_charges', 'sales_tax', 'payment_processing_charges', 'fulfillment_labor_materials', 'total_price_excluding_optional_support', 'total_price_including_optional_support', 'students_reached', 'total_donations', 'num_donors', 'eligible_double_your_impact_match', 'eligible_almost_home_match', 'funding_status', 'date_posted', 'date_completed', 'date_thank_you_packet_mailed', 'date_expiration'])\n",
    "donations = pd.read_csv('./data/opendata_donations000.gz', escapechar='\\\\', names=['donationid', 'projectid', 'donor_acctid', 'cartid', 'donor_city', 'donor_state', 'donor_zip', 'is_teacher_acct', 'donation_timestamp', 'donation_to_project', 'donation_optional_support', 'donation_total', 'donation_included_optional_support', 'payment_method', 'payment_included_acct_credit', 'payment_included_campaign_gift_card', 'payment_included_web_purchased_gift_card', 'payment_was_promo_matched', 'is_teacher_referred', 'giving_page_id', 'giving_page_type', 'for_honoree', 'thank_you_packet_mailed'])\n",
    "resources = pd.read_csv('./data/opendata_resources000.gz', escapechar='\\\\', names=['resourceid', 'projectid', 'vendorid', 'vendor_name', 'item_name', 'item_number', 'item_unit_price', 'item_quantity'])\n",
    "end = perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Cleanup\n",
    "# replace nan with ''\n",
    "print(\"Cleaning Data\")\n",
    "start = perf_counter()\n",
    "projects = projects.fillna('')\n",
    "donations = donations.fillna('')\n",
    "resources = resources.fillna('')\n",
    "\n",
    "#  Clean up column names: remove _ at the start of column name\n",
    "donations.columns = donations.columns.map(lambda x: re.sub('^ ', '', x))\n",
    "donations.columns = donations.columns.map(lambda x: re.sub('^_', '', x))\n",
    "projects.columns = projects.columns.map(lambda x: re.sub('^_', '', x))\n",
    "resources.columns = resources.columns.map(lambda x: re.sub('^ ', '', x))\n",
    "resources.columns = resources.columns.map(lambda x: re.sub('^_', '', x))\n",
    "\n",
    "# Add quotes around projectid values to match format in projects / donations column\n",
    "resources['projectid'] = resources['projectid'].map(lambda x: '\"' + x +'\"')\n",
    "\n",
    "# Add resource_prefix to column names\n",
    "resources.rename(columns={'vendorid': 'resource_vendorid', 'vendor_name': 'resource_vendor_name', 'item_name': 'resource_item_name',\n",
    "       'item_number' :'resource_item_number', \"item_unit_price\": 'resource_item_unit_price',\n",
    "       'item_quantity': 'resource_item_quantity'}, inplace=True)\n",
    "end = perf_counter()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Merge multiple resource row per projectid into a single row\n",
    "# NOTE: section may take a few minutes to execute\n",
    "# this parallel path reduces stage time from 920sec to 169sec on multi-core 2ghz machine\n",
    "print(\"Grouping Data by ProjectId parallel\")\n",
    "start = perf_counter()\n",
    "\n",
    "concat_resource = pd.DataFrame()\n",
    "# a DataFrameGroupBy\n",
    "resources_grouped_by_projectid = resources.groupby('projectid')\n",
    "\n",
    "# return a tuple we can assign\n",
    "def do_concat_by_index(index):\n",
    "    print(\"starting : \"+index)\n",
    "    return (index, resources_grouped_by_projectid[index].apply(lambda x: concatdf(x)))\n",
    "\n",
    "indexes = resources.columns.values\n",
    "print ('Manipulating : {}'.format(indexes))\n",
    "# pool size could be 8 the number in of tasks we need\n",
    "with Pool(10) as pool:\n",
    "    our_result = pool.starmap(do_concat_by_index, zip(indexes))\n",
    "    \n",
    "# move the results from the return list into concat_result\n",
    "for one_result in our_result:\n",
    "    # print('one result index is {} '.format(one_result[0]))\n",
    "    concat_resource[one_result[0]]=one_result[1]\n",
    " \n",
    "concat_resource['projectid'] = concat_resource.index;\n",
    "concat_resource.reset_index(drop=True);\n",
    "concat_resource.index.name = None\n",
    "concat_resource.set_index('projectid', inplace=True, drop=True)\n",
    "\n",
    "end = perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename Project columns\n",
    "print(\"Renaming project columns\")\n",
    "start = perf_counter()\n",
    "\n",
    "projects.rename(columns=lambda x: \"project_\" + x, inplace=True)\n",
    "projects.rename(columns={\"project_projectid\": \"projectid\"}, inplace=True)\n",
    "projects.columns.values\n",
    "projects.index.name = None\n",
    "projects.set_index('projectid', inplace=True, drop=True)\n",
    "\n",
    "end = perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge data into single frame\n",
    "print(\"Merging datasets\")\n",
    "start = perf_counter()\n",
    "data = pd.merge(projects, concat_resource, how='left', right_on='projectid', left_on='projectid')\n",
    "data = pd.merge(donations, data, how='left', right_on='projectid', left_on='projectid')\n",
    "data = data.fillna('')\n",
    "# print('number of records generated to be indexed : {}'.format(len(data)))\n",
    "end = perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Process columns\n",
    "# Modify date formats\n",
    "# moving to parallel execution took us from 1450sec to 380sec\n",
    "print(\"Modifying Date Formats\")\n",
    "start = perf_counter()\n",
    "\n",
    "def do_date_fix(some_data):\n",
    "    #print(some_data.describe())\n",
    "    some_data['project_date_expiration'] = some_data['project_date_expiration'].map(lambda x: str_to_iso(x));\n",
    "    some_data['project_date_posted'] = some_data['project_date_posted'].map(lambda x: str_to_iso(x))\n",
    "    some_data['project_date_thank_you_packet_mailed'] = some_data['project_date_thank_you_packet_mailed'].map(lambda x: str_to_iso(x))\n",
    "    some_data['project_date_completed'] = some_data['project_date_completed'].map(lambda x: str_to_iso(x))\n",
    "    some_data['donation_timestamp'] = some_data['donation_timestamp'].map(lambda x: str_to_iso(x))\n",
    "\n",
    "    # Create location field that combines lat/lon information\n",
    "    some_data['project_location'] = some_data[['project_school_longitude','project_school_latitude']].values.tolist()\n",
    "    del(some_data['project_school_latitude'])  # delete latitude field\n",
    "    del(some_data['project_school_longitude']) # delete longitude\n",
    "    return some_data\n",
    "\n",
    "num_workers = 8\n",
    "data_split = np.array_split(data,num_workers)\n",
    "with Pool(num_workers) as pool:\n",
    "    fixed_dates = pool.map(do_date_fix,data_split)\n",
    "data = pd.concat(fixed_dates)\n",
    "\n",
    "end = perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Create and configure Elasticsearch index\n",
    "print(\"Preparing to Index to ES\")\n",
    "start = perf_counter()\n",
    "# Name of index and document type\n",
    "index_name = 'donorschoose'\n",
    "doc_name = 'donation'\n",
    "\n",
    "# Delete donorschoose index if one does exist\n",
    "if es.indices.exists(index_name):\n",
    "    es.indices.delete(index_name)\n",
    "\n",
    "# Create donorschoose index\n",
    "es.indices.create(index_name)\n",
    "\n",
    "# Add mapping\n",
    "with open('donorschoose_mapping.json') as json_mapping:\n",
    "    d = json.load(json_mapping)\n",
    "\n",
    "es.indices.put_mapping(index=index_name, doc_type=doc_name, body=d, include_type_name=True)\n",
    "end = perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function used by all below\n",
    "def read_data(df):\n",
    "    for don_id, thisDonation in df.iterrows():\n",
    "        # print every 10000 iteration\n",
    "        if don_id % 10000 == 0:\n",
    "            print('{} / {}'.format(don_id, len(df.index) ))\n",
    "        doc={}\n",
    "        doc[\"_index\"]=index_name\n",
    "        doc[\"_id\"]=thisDonation['donationid']\n",
    "        doc[\"_type\"]=doc_name\n",
    "        doc[\"_source\"]=thisDonation.to_dict()\n",
    "        if don_id % 100000 == 0:\n",
    "            print('doc: {}'.format(doc))\n",
    "        yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution blocks\n",
    "Only run one of these three options\n",
    "* es parallel-bulk\n",
    "* es bulk\n",
    "* Pool es bulk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parallel bulk - sometimes exits early on last chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Index Data into Elasticsearch - parallel bulk - default parallel_bulk thread_count = 4\n",
    "print(\"Indexing parallel_bulk\")\n",
    "start = perf_counter()\n",
    "# parallel_bulk returns generators which must be consumed https://elasticsearch-py.readthedocs.io/en/master/helpers.html\n",
    "# default request_timeout=10\n",
    "# 1000 may have timeout\n",
    "for success, info in helpers.parallel_bulk(es, read_data(data),thread_count=8, request_timeout=20.0, chunk_size=500, index=index_name,doc_type=doc_name):\n",
    "    if not success:\n",
    "        print('A document failed:', info)\n",
    "\n",
    "end = perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standard bulk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index Data into Elasticsearch\n",
    "print(\"Indexing bulk\")\n",
    "start = perf_counter()\n",
    "helpers.bulk(es,read_data(data), index=index_name,doc_type=doc_name)\n",
    "end = perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool Execution\n",
    "run both of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this if using the pool\n",
    "print(\"Chunking for pool\")\n",
    "start = perf_counter()\n",
    "# create as many processes as there are CPUs on your machine - leave one for everyone else\n",
    "num_partitions = multiprocessing.cpu_count() - 1\n",
    "num_partitions = 8\n",
    "chunks = np.array_split(data, num_partitions)\n",
    "print('chunk count {}'.format(len(chunks)))\n",
    "end = perf_counter()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index Data into Elasticsearch - pool approach with `num_partitions` processes\n",
    "pool = Pool(processes=num_partitions)\n",
    "\n",
    "def es_pool_func(aChunk):\n",
    "    print('chunked es bulk : {}'.format(len(aChunk)))\n",
    "    es = Elasticsearch()\n",
    "    helpers.bulk(es,read_data(aChunk), index=index_name,doc_type=doc_name)\n",
    "\n",
    "print(\"Indexing Chunked\")\n",
    "start = perf_counter()\n",
    "# apply our function to each chunk in the list\n",
    "# with multiprocessing.Pool(processes=num_partitions) as pool:\n",
    "#     result = pool.map(es_pool_func, chunks)\n",
    "\n",
    "pool = Pool(num_partitions)\n",
    "for aChunk in chunks:\n",
    "    pool.apply_async(es_pool_func, args=(aChunk,))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "end = perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# playground blocks used to try out various cluster API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = es.update_by_query(index=index_name,doc_type=doc_name, request_timeout=30.0, )"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
