{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting flight delays with regression analysis\n",
    "Let’s try to predict flight delays by using the sample flight data. We want to be able to use information such as weather and location of the destination and origin, flight distance and carrier to predict the number of minutes delayed for each flight. As it is a continuous numeric variable, we’ll use regression analysis to make the prediction.\n",
    "\n",
    "We have chosen this dataset as an example because it is easily accessible for Kibana users and the use case is relevant. However, the data has been manually created and contains some inconsistencies. For example, a flight can be both delayed and canceled. Please remember that the quality of your input data will affect the quality of results.\n",
    "\n",
    "Each document in the dataset contains details for a single flight, so this data is ready for analysis as it is already in a two-dimensional entity-based data structure (data frame). In general, you often need to transform the data into an entity-centric index before you analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import pprint\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "import requests\n",
    "## create a client to connect to Elasticsearch\n",
    "es_url = 'http://localhost:9200'\n",
    "es_client = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hits': {'hits': [{'_index': 'kibana_sample_data_flights',\n",
       "    '_type': '_doc',\n",
       "    '_id': 'P3SlOG8BYfkX4eTi_ex6',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'FlightNum': '9HY9SWR',\n",
       "     'DestCountry': 'AU',\n",
       "     'OriginWeather': 'Sunny',\n",
       "     'OriginCityName': 'Frankfurt am Main',\n",
       "     'AvgTicketPrice': 841.2656419677076,\n",
       "     'DistanceMiles': 10247.856675613455,\n",
       "     'FlightDelay': False,\n",
       "     'DestWeather': 'Rain',\n",
       "     'Dest': 'Sydney Kingsford Smith International Airport',\n",
       "     'FlightDelayType': 'No Delay',\n",
       "     'OriginCountry': 'DE',\n",
       "     'dayOfWeek': 0,\n",
       "     'DistanceKilometers': 16492.32665375846,\n",
       "     'timestamp': '2019-12-16T00:00:00',\n",
       "     'DestLocation': {'lat': '-33.94609833', 'lon': '151.177002'},\n",
       "     'DestAirportID': 'SYD',\n",
       "     'Carrier': 'Kibana Airlines',\n",
       "     'Cancelled': False,\n",
       "     'FlightTimeMin': 1030.7704158599038,\n",
       "     'Origin': 'Frankfurt am Main Airport',\n",
       "     'OriginLocation': {'lat': '50.033333', 'lon': '8.570556'},\n",
       "     'DestRegion': 'SE-BD',\n",
       "     'OriginAirportID': 'FRA',\n",
       "     'OriginRegion': 'DE-HE',\n",
       "     'DestCityName': 'Sydney',\n",
       "     'FlightTimeHour': 17.179506930998397,\n",
       "     'FlightDelayMin': 0}}]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## insert example of reading docs from ES index\n",
    "\n",
    "results = es_client.search(index='kibana_sample_data_flights', filter_path=['hits.hits._*'], size=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is a supervised machine learning analysis and therefore needs to train on data that contains the ground truth for the dependent_variable that we want to predict. In this example, the ground truth is available in each document as the actual value of FlightDelayMins. In order to be analyzed, a document must contain at least one field with a supported data type (numeric, boolean, text, keyword or ip) and must not contain arrays with more than one item.\n",
    "\n",
    "If your source data consists of some documents that contain a dependent_variable and some that do not, the model is trained on the training_percent of the documents that contain ground truth. However, predictions are made against all of the data. The current implementation of regression analysis supports a single batch analysis for both training and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a regression model\n",
    "To predict the number of minutes delayed for each flight:\n",
    "Create a data frame analytics job.\n",
    "Use the create data frame analytics jobs API as you can see in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'no handler found for uri '\n",
      "          '[/_ml/data_frame/analytics/model-flight-delays] and method [PUT]'}\n"
     ]
    }
   ],
   "source": [
    "endpoint_url = \"/_ml/data_frame/analytics/model-flight-delays\"\n",
    "\n",
    "job_config = {\n",
    "  \"source\": {\n",
    "    \"index\": [\n",
    "      \"kibana_sample_data_flights\" # [1]\n",
    "    ],\n",
    "    \"query\": { \n",
    "      \"range\": {\n",
    "        \"DistanceKilometers\": {  # [2]\n",
    "          \"gt\": 0\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"dest\": {\n",
    "    \"index\": \"df-flight-delays\"  # [3]\n",
    "  },\n",
    "  \"analysis\": {\n",
    "    \"regression\": {\n",
    "      \"dependent_variable\": \"FlightDelayMin\",  # [4]\n",
    "      \"training_percent\": 90  #  [5] see below note on training percent\n",
    "    }\n",
    "  },\n",
    "  \"analyzed_fields\": {\n",
    "    \"includes\": [],\n",
    "    \"excludes\": [     # [6]\n",
    "      \"Cancelled\",\n",
    "      \"FlightDelay\",\n",
    "      \"FlightDelayType\"\n",
    "    ]\n",
    "  },\n",
    "  \"model_memory_limit\": \"100mb\"  # [7]\n",
    "}\n",
    "\n",
    "result = requests.put(es_url+endpoint_url, json=job_config)\n",
    "pprint.pprint(result.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] The source index to analyze.\n",
    "\n",
    "\n",
    "[2] This query removes erroneous data from the analysis to improve its quality.\n",
    "\n",
    "\n",
    "[3] The index that will contain the results of the analysis; it will consist of a copy of the source index data where each document is annotated with the results.\n",
    "\n",
    "\n",
    "[4] Specifies the continuous variable we want to predict with the regression analysis.\n",
    "\n",
    "\n",
    "[5] Specifies the approximate proportion of data that is used for training. In this example we randomly select 90% of the source data for training.\n",
    "\n",
    "\n",
    "[6] Specifies fields to be excluded from the analysis. It is recommended to exclude fields that either contain erroneous data or describe the dependent_variable.\n",
    "\n",
    "\n",
    "[7] Specifies a memory limit for the job. If the job requires more than this amount of memory, it fails to start. This makes it possible to prevent job execution if the available memory on the node is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A brief note on training percentage\n",
    "\n",
    "As you may have noticed, in the job configuration above we set the value of `training_percent` to 90. This means that out of the whole Flights dataset 90 percent of the data will be used to train model and the remaining 10 percent of the data will be used for testing the model. \n",
    "You might wonder at this point, what is the best percentage for the train/test split and how you should choose what percentage to use in your own job? The answer will usually depend on your particular situation. In general it is useful to consider some of the following tradeoffs.\n",
    "The more data you supply to the model at training time, the more examples the model will have to learn from, which usually leads to a better classification performance. However, more training data will also increase the training time of the model and at some point, providing the model with more training examples will only result in marginal increase in accuracy. \n",
    "\n",
    "Moreover, the more data you use for training, the less data you have for the testing phase. This means that you will have less previously unseen examples to show your model and thus perhaps your estimate for the generalization error will not be as accurate. \n",
    "\n",
    "In general, for datasets containing several thousand docs or more, start with a low 5-10% training percentage and see how your results and runtime evolve as you increase the training percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Start the job\n",
    "\n",
    "start_endpoint = \"/_ml/data_frame/analytics/model-flight-delays/_start\"\n",
    "result = requests.post(es_url+start_endpoint)\n",
    "pprint.pprint(result.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job takes a few minutes to run. Runtime depends on the local hardware and also on the number of documents and fields that analyzed. The more fields and documents, the longer the job runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check the job stats\n",
    "\n",
    "stats_endpoint = \"/_ml/data_frame/analytics/model-flight-delay-delays/_stats\"\n",
    "result = requests.get(es_url+stats_endpoint)\n",
    "pprint.pprint(result.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Regression Results\n",
    "Now you have a new index that contains a copy of your source data with predictions for your dependent variable. Use the standard Elasticsearch search command to view the results in the destination index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code to get results\n",
    "query = {\"query\": {\"term\": {\"ml.is_training\": {\"value\": False }}}}\n",
    "result = es_client.search(index='df-flight-delays', filter_path=['hits.hits._*'], size=1, body=query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Results\n",
    "The results can be evaluated for documents which contain both the ground truth field and the prediction. In the example below, FlightDelayMins contains the ground truth and the prediction is stored as ml.FlightDelayMin_prediction.\n",
    "\n",
    "Use the data frame analytics evaluate API to evaluate the results.\n",
    "\n",
    "First, we want to know the training error that represents how well the model performed on the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the training error\n",
    "\n",
    "evaluate_endpoint = \"/_ml/data_frame/_evaluate\"\n",
    "\n",
    "config = {\n",
    " \"index\": \"df-flight-delays\",  # [1]\n",
    "   \"query\": {\n",
    "    \"term\": {\n",
    "      \"ml.is_training\": {  # [2]\n",
    "        \"value\": True  \n",
    "      }\n",
    "    }\n",
    "  },\n",
    " \"evaluation\": {\n",
    "   \"regression\": {\n",
    "     \"actual_field\": \"FlightDelayMin\",   # [3]\n",
    "     \"predicted_field\": \"ml.FlightDelayMin_prediction\", # [4]\n",
    "     \"metrics\": {\n",
    "       \"r_squared\": {},\n",
    "       \"mean_squared_error\": {}\n",
    "     }\n",
    "   }\n",
    " }\n",
    "}\n",
    "\n",
    "result = requests.post(es_url+evaluate_endpoint, json=config)\n",
    "result.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] The destination index which is the output of the analysis job.\n",
    "\n",
    "[2] We calculate the training error by only evaluating the training data.\n",
    "\n",
    "[3] The ground truth label.\n",
    "\n",
    "[4] Predicted value.\n",
    "\n",
    "Next, we would like to compute the generalisation error - that is, how well the model performs on data points that have not been used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the training error\n",
    "\n",
    "evaluate_endpoint = \"/_ml/data_frame/_evaluate\"\n",
    "\n",
    "config = {\n",
    " \"index\": \"df-flight-delays\",  # [1]\n",
    "   \"query\": {\n",
    "    \"term\": {\n",
    "      \"ml.is_training\": {  # [2]\n",
    "        \"value\": False  \n",
    "      }\n",
    "    }\n",
    "  },\n",
    " \"evaluation\": {\n",
    "   \"regression\": {\n",
    "     \"actual_field\": \"FlightDelayMin\",   # [3]\n",
    "     \"predicted_field\": \"ml.FlightDelayMin_prediction\", # [4]\n",
    "     \"metrics\": {\n",
    "       \"r_squared\": {},\n",
    "       \"mean_squared_error\": {}\n",
    "     }\n",
    "   }\n",
    " }\n",
    "}\n",
    "\n",
    "result = requests.post(es_url+evaluate_endpoint, json=config)\n",
    "result.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
